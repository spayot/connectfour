# ConnectFour x AlphaZero
*Hobby Project during my visa-related forced unpaid leave in spring 2020!!*
*Refactored in Spring 2022*

- AlphaGo Paper: https://arxiv.org/abs/1712.01815

**GOAL**: train an agent to play Connect 4 using a strategy similar to AlphaGo Zero.
**STRATEGY**: instead of creating a brute-force mimimax strategy (exhaustive tree search + feature engineering), we adopt a less resource-intensive approach: truncated tree search. This strategy consists in:
- an evaluator predicting the probabilities for each possible next move to be the best action as well as the overall probability of winning for the next player. This evaluator is essentially based on a classic CNN architecture (ResNet) with 2 heads: a policy head with a softmax actgivation and a state value head with a sigmoid activation.
- a version of Monte-Carlo Tree Search that leverages this evaluator to prioritize the regions of the tree to explore further, and returns an improved policy compared to the original evaluator.  
The evaluator is iteratively trained following batches of self-played games. Training data is gathered by using:
- the improved policy output from MCTS for the policy head and
- the final outcome of the game (-1,0,1) for the value head.  
```mermaid
flowchart:
A --> B
```
**SOME VERY VALUABLE READING**  
- [lessons from implementing alphazero](https://medium.com/oracledevs/lessons-from-implementing-alphazero-7e36e9054191)
- [MCTS in AlphaGoZero](https://medium.com/@jonathan_hui/monte-carlo-tree-search-mcts-in-alphago-zero-8a403588276a)  
The former explains some implementation details and offers insights into good hyper-parameters to fine-tune a model for Connect Four:  
- **Self-play$MCTS$:**|
    - $c_{puct} = 4$ : coefficient for the $U(s,a)$ value for the PUCT version of the UCB value.
    - $\alpha = 1$: Dirichlet distribution parameter defining the level of noise introduced into the prior probability for actions from the root node (encouraging more epxloration)
    - $\tau = 1$: modifies the way the improved policy $\Pi$ is estimated. the higher $\tau$ is, the more flattened the policy distribution is going to get, promoting more exploration, but also leading to more frequent suboptimal moves. when $\tau$ tends to zero,
- **Policy Evaluator (Neural Network):**
    - Core Model:
        - Paper: ResNet 20 x 256 filters (24M params)
        - Original Implementation: ResNet 5 layers x 64 filters (386k parameters)
        - Later: ResNet 20 layers x 128 filters  (6.3M paramaters)
    - Value and Policy heads:
        - originally: resp. 1 and 2 filters
        - later: 32 filters
- **Generating Training Data:**
    - 7168 self-play games per generation (140K-220K game positions to evaluate)
    - 40 generations to plateau performance
    - training target for value head: average of $z$ and root node's $Q$ instead of only $z$.
    - position averaging: de-duplicate positions frequenty encountered in the training data and replace $Pi$ and $z$ with their averages.    
- **Training Pipeline**
    - sampling window: grows from 4 last gens to 20 last gens (after 35 epochs)
    - sample size: 500,000
    - epochs: 2 per generation
    - $lr$ learning rate: 1cycle learning rate
   
**STEPS:**
- [x] setup an environment to:  
  - [x] define state transitions
  - [x] define possible moves
  - [x] define game result
- [x] render states
- [x] create estimator: simple CNN with
    - [x] a policy head (softmax across 7 outputs)
    - [x] a value head estimate value of any state (between -1 and 1) > tanh
- [x] training:
    - [x] self-play for n games:
        - sample from potential moves, using estimator policy head distribution
        - perform MCTS to search the game space given a search "budget" (N evaluations, X seconds, ...) > needs to modify mctspy code: currently relies on random rollouts, no estimator / feature extractor to determine best moves.
    - [x] then: back-propagate final outcome of games (RL) > Vi, Qi
    - [x] use $V_i = Q_i + z$  and $\pi_i$ as labels to train estimator
    - [x] augmentation: use symmetry of the game to multiply by 2 the number of training samples generated by a single game.
